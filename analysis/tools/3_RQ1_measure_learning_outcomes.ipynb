{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook computes the learning outcomes from the pre-test and post-test responses.\n",
    "\n",
    "We consider two learning goals:\n",
    "* LG1: identify a (feasible) solution to the problem\n",
    "* LG2: construct a correct (i.e. optimal) solution.\n",
    "    \n",
    "For LG1, we check connectedness, i.e. if solutions are spanning or not\n",
    "out.\n",
    "\n",
    "For LG2, we check optimality, i.e. how close was the solution to the correct solution, quantified by the error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import copy\n",
    "import pickle\n",
    "\n",
    "import pathlib as pl\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "from scipy.stats import wilcoxon, shapiro, ttest_rel\n",
    "from effsize.effsize import two_group_difference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define paths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "transition_tables_pickle_file = pl.Path(\n",
    "    '../processed_data/justhink_spring21_transition_tables.pickle')\n",
    "\n",
    "learning_pickle_file = pl.Path(\n",
    "    '../processed_data/learning_table.pickle')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load transition tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with transition_tables_pickle_file.open('rb') as handle:\n",
    "    transition_tables = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transition_tables.keys()\n",
    "# pd.options.display.max_rows = None\n",
    "# transition_tables[10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- ### Cleaning tables. -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fiter for the submissions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing for participant 1...\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "Index(['header.frame_id'], dtype='object')",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_40445/1670943036.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m \u001b[0msubmission_tables\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfilter_submissions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtransition_tables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_40445/1670943036.py\u001b[0m in \u001b[0;36mfilter_submissions\u001b[0;34m(transition_tables, verbose)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0;31m# Remove duplicate rows from submission log and keeping the last submission\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m         df.drop_duplicates(subset=\"header.frame_id\",\n\u001b[0m\u001b[1;32m     20\u001b[0m                            keep='last', inplace=True)\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/playground/data-justhink-spring21/venv/lib/python3.8/site-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    309\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m                 )\n\u001b[0;32m--> 311\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    312\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/playground/data-justhink-spring21/venv/lib/python3.8/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36mdrop_duplicates\u001b[0;34m(self, subset, keep, inplace, ignore_index)\u001b[0m\n\u001b[1;32m   6056\u001b[0m         \u001b[0minplace\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalidate_bool_kwarg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minplace\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"inplace\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6057\u001b[0m         \u001b[0mignore_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalidate_bool_kwarg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mignore_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"ignore_index\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6058\u001b[0;31m         \u001b[0mduplicated\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mduplicated\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkeep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   6059\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6060\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mduplicated\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/playground/data-justhink-spring21/venv/lib/python3.8/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36mduplicated\u001b[0;34m(self, subset, keep)\u001b[0m\n\u001b[1;32m   6190\u001b[0m         \u001b[0mdiff\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mIndex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdifference\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6191\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mdiff\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mempty\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6192\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdiff\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   6193\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6194\u001b[0m         \u001b[0mvals\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcol\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msubset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: Index(['header.frame_id'], dtype='object')"
     ]
    }
   ],
   "source": [
    "def filter_submissions(transition_tables, verbose=False):\n",
    "    \"\"\"Get the submission states only, and drop the duplicates, \n",
    "    unused columns, keep state, activity, imported info.\"\"\"\n",
    "    tables = {}\n",
    "    \n",
    "    for participant in sorted(transition_tables):\n",
    "        print('Processing for participant {}...'.format(participant))\n",
    "        \n",
    "        df = transition_tables[participant].copy()\n",
    "        \n",
    "        # Filter for the submission rows.\n",
    "        df = df[df['is_submission']]\n",
    "\n",
    "        # Remove. collaborative activity rows\n",
    "        # df = df[df['header.frame_id'] != \"collab-activity\"]\n",
    "        # df = df[df['header.frame_id'] != \"collab-activity-2\"]\n",
    "\n",
    "        # Remove duplicate rows from submission log and keeping the last submission\n",
    "        df.drop_duplicates(subset='activity', keep='last', inplace=True)\n",
    "\n",
    "        mst_costs = []\n",
    "        spanning = []\n",
    "        norm_error = []\n",
    "\n",
    "        for i, row in df.iterrows():\n",
    "            cost = row['state'].network.get_mst_cost()\n",
    "            span = row['state'].network.is_spanning()\n",
    "            mst_costs.append(cost)\n",
    "            spanning.append(span)\n",
    "            # compute normalized error\n",
    "            if span:\n",
    "                opt_cost = row['state'].network.get_mst_cost()\n",
    "                cost = row['cost']\n",
    "                error = (cost - opt_cost) / opt_cost\n",
    "                norm_error.append(error)\n",
    "            else:\n",
    "                norm_error.append(None)\n",
    "\n",
    "        # adding mst_cost, spanning, and normalized_error columns\n",
    "        df['mst_cost'] = mst_costs\n",
    "        df['spanning'] = spanning\n",
    "        df['error'] = norm_error\n",
    "\n",
    "        tables[participant] = df\n",
    "\n",
    "        if verbose:\n",
    "            display(df)\n",
    "\n",
    "    return tables\n",
    "\n",
    "\n",
    "submission_tables = filter_submissions(transition_tables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_tables[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check connectivity, optimality, and compute average error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_spanning_score(df):\n",
    "    \"\"\"Calculate spanning score for pre-test and post-test.\"\"\"\n",
    "    pre_spanning_score = 0\n",
    "    post_spanning_score = 0\n",
    "    for index, row in df.iterrows():\n",
    "        if 'pre' in row['header.frame_id']:\n",
    "            if row['spanning']:\n",
    "                pre_spanning_score += 1\n",
    "        elif 'post' in row['header.frame_id']:\n",
    "            if row['spanning']:\n",
    "                post_spanning_score += 1\n",
    "    return pre_spanning_score, post_spanning_score\n",
    "\n",
    "\n",
    "for key, table in all_tables.items():\n",
    "    print('participant', key)\n",
    "    print('spanning scores: ', compute_spanning_score(table))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_mst_score(df):\n",
    "    \"\"\"Compute is_mst score for pre-test and post-test.\"\"\"\n",
    "    pre_mst_score = 0\n",
    "    post_mst_score = 0\n",
    "    for index, row in df.iterrows():\n",
    "        if 'pre' in row['header.frame_id']:\n",
    "            if row['is_mst']:\n",
    "                pre_mst_score += 1\n",
    "        elif 'post' in row['header.frame_id']:\n",
    "            if row['is_mst']:\n",
    "                post_mst_score += 1\n",
    "    return pre_mst_score, post_mst_score\n",
    "\n",
    "for key, table in all_tables.items():\n",
    "    print('student', key)\n",
    "    print('mst scores: ',compute_mst_score(table))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_average_error(df):\n",
    "    \"\"\"Compute average error for pre-test and post-test.\"\"\"\n",
    "    pre_total = 0\n",
    "    post_total = 0\n",
    "    pre_count = 0\n",
    "    post_count = 0\n",
    "    for index, row in df.iterrows():\n",
    "        value = row['error']\n",
    "        if 'pre' in row['header.frame_id']:\n",
    "            # not computing the error for submissions that are not feasbile/spanning\n",
    "            if not math.isnan(value):\n",
    "                pre_total += value\n",
    "                pre_count += 1\n",
    "        elif 'post' in row['header.frame_id']:\n",
    "            # not computing the error for submissions that are not feasbile/spanning\n",
    "            if not math.isnan(value):\n",
    "                post_total += value\n",
    "                post_count += 1\n",
    "\n",
    "    # compute averages while handling zero division\n",
    "    if pre_count != 0:\n",
    "        pre_avg = pre_total/pre_count\n",
    "    else:\n",
    "        pre_avg = None\n",
    "    if post_count != 0:\n",
    "        post_avg = post_total/post_count\n",
    "    else:\n",
    "        post_avg = None\n",
    "    return pre_avg, post_avg\n",
    "\n",
    "\n",
    "for participant, table in all_tables.items():\n",
    "    print('participant {} pre- post- average errors = {}'.format(\n",
    "        participant, compute_average_error(table)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combine all outcomes into a summary table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "learn_df = pd.DataFrame()\n",
    "learn_df['participant'] = all_tables.keys()\n",
    "\n",
    "pre_span = list()\n",
    "post_span = list()\n",
    "pre_mst = list()\n",
    "post_mst = list()\n",
    "pre_error = list()\n",
    "post_error = list()\n",
    "\n",
    "# computing scores and appending them to the master table\n",
    "for participant, table in all_tables.items():\n",
    "    pre_span_val, post_span_val = compute_spanning_score(table)\n",
    "    pre_mst_val, post_mst_val = compute_mst_score(table)\n",
    "    pre_error_avg, post_error_avg = compute_average_error(table)\n",
    "    pre_span.append(pre_span_val)\n",
    "    post_span.append(post_span_val)\n",
    "    pre_mst.append(pre_mst_val)\n",
    "    post_mst.append(post_mst_val)\n",
    "    pre_error.append(pre_error_avg)\n",
    "    post_error.append(post_error_avg)\n",
    "\n",
    "learn_df['pretest_span'] = pre_span\n",
    "learn_df['posttest_span'] = post_span\n",
    "learn_df['pretest_mst'] = pre_mst\n",
    "learn_df['posttest_mst'] = post_mst\n",
    "learn_df['pre_error'] = pre_error\n",
    "learn_df['post_error'] = post_error\n",
    "\n",
    "# sorting table by participant.\n",
    "learn_df.sort_values(by=['participant'], inplace=True)\n",
    "\n",
    "display(learn_df)\n",
    "\n",
    "#premin error\n",
    "#pre max error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reshape tables for visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# reformatting is_spanning and is_mst dataframes for visualization\n",
    "span_learning_df = learn_df[['participant', 'pretest_span', 'posttest_span']]\n",
    "mst_learning_df = learn_df[['participant', 'pretest_mst', 'posttest_mst']]\n",
    "error_learning_df = learn_df[['participant', 'pre_error', 'post_error']]\n",
    "\n",
    "spandf = pd.DataFrame(np.repeat(span_learning_df.values, 2, axis=0))\n",
    "spandf.columns = span_learning_df.columns\n",
    "\n",
    "mstdf = pd.DataFrame(np.repeat(mst_learning_df.values, 2, axis=0))\n",
    "mstdf.columns = mst_learning_df.columns\n",
    "\n",
    "errordf = pd.DataFrame(np.repeat(error_learning_df.values, 2, axis=0))\n",
    "errordf.columns = error_learning_df.columns\n",
    "\n",
    "# reformatting spanning dataframe\n",
    "p_type = list()\n",
    "value = list()\n",
    "for index, row in spandf.iterrows():\n",
    "    if index % 2 == 0:\n",
    "        p_type.append('pretest')\n",
    "        value.append(row['pretest_span'])\n",
    "    else:\n",
    "        p_type.append('posttest')\n",
    "        value.append(row['posttest_span'])\n",
    "spandf['p_type'] = p_type\n",
    "spandf['value'] = value\n",
    "\n",
    "# reformatting mst dataframe\n",
    "mst_value = list()\n",
    "for index, row in mstdf.iterrows():\n",
    "    if index % 2 == 0:\n",
    "        mst_value.append(row['pretest_mst'])\n",
    "    else:\n",
    "        mst_value.append(row['posttest_mst'])\n",
    "mstdf['p_type'] = p_type\n",
    "mstdf['value'] = mst_value\n",
    "\n",
    "# reformatting error dataframe\n",
    "error_value = list()\n",
    "for index, row in errordf.iterrows():\n",
    "    if index % 2 == 0:\n",
    "        error_value.append(row['pre_error'])\n",
    "    else:\n",
    "        error_value.append(row['post_error'])\n",
    "errordf['p_type'] = p_type\n",
    "errordf['value'] = error_value\n",
    "\n",
    "\n",
    "spandf.drop(columns=['pretest_span', 'posttest_span'], inplace=True)\n",
    "mstdf.drop(columns=['pretest_mst', 'posttest_mst'], inplace=True)\n",
    "errordf.drop(columns=['pre_error', 'post_error'], inplace=True)\n",
    "\n",
    "display(spandf)\n",
    "display(mstdf)\n",
    "display(errordf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reshape collaborative activity table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# formatting dataframe for collab activities\n",
    "cdf = pd.DataFrame()\n",
    "\n",
    "for key, mdf in all_tables.items():\n",
    "    collab_df = mdf.loc[mdf['header.frame_id'].isin(\n",
    "        ['collab-activity', 'collab-activity-2'])].copy()\n",
    "    collab_df['participant'] = key\n",
    "    collab_df.drop(columns=['action.agent_name', 'state', 'is_submission',\n",
    "                   'cost', 'mst_cost', 'spanning', 'error'], inplace=True)\n",
    "    collab_df.rename(columns={\"header.frame_id\": \"activity\"}, inplace=True)\n",
    "    cdf = cdf.append(collab_df)\n",
    "\n",
    "cdf.sort_values(by=['participant'], inplace=True)\n",
    "values = list()\n",
    "for index, row in cdf.iterrows():\n",
    "    if row['is_mst'] == True:\n",
    "        values.append(1)\n",
    "    else:\n",
    "        values.append(0)\n",
    "cdf['is_mst'] = values\n",
    "cdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize collaborative activity performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "participants = [1, 2, 3, 4, 5, 6, 7, 9, 10]\n",
    "separation = 0.01\n",
    "\n",
    "# graphing the pretest versus posttest is_spanning scores\n",
    "fig, ax = plt.subplots(1, figsize=(5, 5))\n",
    "\n",
    "prelist = list()\n",
    "postlist = list()\n",
    "for i in participants:\n",
    "    temp = cdf[cdf['participant'] == i]\n",
    "    values = list(temp.is_mst)\n",
    "    #computing offset between overlapping lines\n",
    "    p0 = values[0] + prelist.count(values[0])*separation\n",
    "    p1 = values[1] + prelist.count(values[1])*separation\n",
    "    prelist.append(values[0])\n",
    "    postlist.append(values[1])\n",
    "    #plotting\n",
    "    plt.plot(temp.activity, [p0,p1], marker='o', markersize=5)\n",
    "plt.ylabel('Score')\n",
    "plt.title('Collab Activity Success\\n', loc='center', fontsize=20)\n",
    "\n",
    "leg = plt.legend(participants, loc='upper left', frameon=True)\n",
    "# get the bounding box of the original legend\n",
    "bb = leg.get_bbox_to_anchor().inverse_transformed(ax.transAxes)\n",
    "# change to location of the legend\n",
    "xOffset = 1.1\n",
    "bb.x0 += xOffset\n",
    "bb.x1 += xOffset\n",
    "leg.set_bbox_to_anchor(bb, transform=ax.transAxes)\n",
    "\n",
    "# set y axis ticks to percentages\n",
    "yticks = plt.yticks()[0]\n",
    "plt.yticks(np.arange(0, 2, step=1), ['Fail','Success'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize pretest and postest learning scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "participants = [1, 2, 3, 4, 5, 6, 7, 9, 10]\n",
    "separation = 0.06\n",
    "\n",
    "# graphing the pretest versus posttest is_spanning scores\n",
    "fig, ax = plt.subplots(1, figsize=(5, 7))\n",
    "\n",
    "prelist = list()\n",
    "postlist = list()\n",
    "for i in participants:\n",
    "    temp = spandf[spandf['participant'] == i]\n",
    "    values = list(temp.value)\n",
    "    #computing offset between overlapping lines\n",
    "    p0 = values[0] + prelist.count(values[0])*separation\n",
    "    p1 = values[1] + prelist.count(values[1])*separation\n",
    "    prelist.append(values[0])\n",
    "    postlist.append(values[1])\n",
    "    #plotting\n",
    "    plt.plot(temp.p_type, [p0,p1], marker='o', markersize=5)\n",
    "plt.ylabel('Score')\n",
    "plt.title('Feasible Solution\\n', loc='center', fontsize=20)\n",
    "\n",
    "\n",
    "leg = plt.legend(participants, loc='upper left', frameon=True)\n",
    "# get the bounding box of the original legend\n",
    "bb = leg.get_bbox_to_anchor().inverse_transformed(ax.transAxes)\n",
    "# change to location of the legend\n",
    "xOffset = 1.1\n",
    "bb.x0 += xOffset\n",
    "bb.x1 += xOffset\n",
    "leg.set_bbox_to_anchor(bb, transform=ax.transAxes)\n",
    "\n",
    "# set y axis ticks to percentages\n",
    "yticks = plt.yticks()[0]\n",
    "plt.yticks(yticks[1:-1], [str(round((i/5)*100)) + '%' for i in yticks[1:-1]])\n",
    "plt.show()\n",
    "\n",
    "# graphing the pretest versus posttest is_mst scores\n",
    "fig, ax = plt.subplots(1, figsize=(5, 7))\n",
    "\n",
    "prelist = list()\n",
    "postlist = list()\n",
    "for i in participants:\n",
    "    temp = mstdf[mstdf['participant'] == i]\n",
    "    values = list(temp.value)\n",
    "    #computing offset between overlapping lines\n",
    "    p0 = values[0] + prelist.count(values[0])*separation\n",
    "    p1 = values[1] + prelist.count(values[1])*separation\n",
    "    prelist.append(values[0])\n",
    "    postlist.append(values[1])\n",
    "    #plotting\n",
    "    plt.plot(temp.p_type, [p0,p1], marker='o', markersize=5)\n",
    "plt.ylabel('Score')\n",
    "plt.title('Optimal Solution\\n', loc='center', fontsize=20)\n",
    "\n",
    "leg = plt.legend(participants, loc='upper left', frameon=True)\n",
    "# get the bounding box of the original legend\n",
    "bb = leg.get_bbox_to_anchor().inverse_transformed(ax.transAxes)\n",
    "# change to location of the legend\n",
    "xOffset = 1.1\n",
    "bb.x0 += xOffset\n",
    "bb.x1 += xOffset\n",
    "leg.set_bbox_to_anchor(bb, transform=ax.transAxes)\n",
    "\n",
    "# set y axis ticks to percentages\n",
    "yticks = plt.yticks()[0]\n",
    "plt.yticks(yticks[1:-1], [str(round((i/5)*100)) + '%' for i in yticks[1:-1]])\n",
    "plt.show()\n",
    "\n",
    "# graphing the pretest versus posttest spanning scores\n",
    "fig, ax = plt.subplots(1, figsize=(5, 7))\n",
    "for i in participants:\n",
    "    temp = errordf[errordf['participant'] == i]\n",
    "    plt.plot(temp.p_type, temp.value, marker='o', markersize=5)\n",
    "plt.ylabel('Score')\n",
    "plt.title('Error Average\\n', loc='center', fontsize=20)\n",
    "\n",
    "leg = plt.legend(participants, loc='upper left', frameon=True)\n",
    "# get the bounding box of the original legend\n",
    "bb = leg.get_bbox_to_anchor().inverse_transformed(ax.transAxes)\n",
    "# change to location of the legend\n",
    "xOffset = 1.1\n",
    "bb.x0 += xOffset\n",
    "bb.x1 += xOffset\n",
    "leg.set_bbox_to_anchor(bb, transform=ax.transAxes)\n",
    "\n",
    "# set y axis ticks to percentages\n",
    "yticks = plt.yticks()[0]\n",
    "plt.yticks(yticks[1:-1], [str(round(i*100)) + '%' for i in yticks[1:-1]])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform Wilcoxon signed-rank test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = learning_df.copy()\n",
    "df.rename(columns={\"pre_error\": \"pretest_error\", \"post_error\":\"posttest_error\"}, inplace=True)\n",
    "display(df)\n",
    "df.dropna(inplace=True)\n",
    "\n",
    "categories = ['span','mst', 'error']\n",
    "\n",
    "for name in categories:\n",
    "    r = list(df['pretest_' + name])\n",
    "    o = list(df['posttest_' + name])\n",
    "    print(name)\n",
    "    w, p = wilcoxon(r,o,mode=\"exact\") \n",
    "    #null hypotehsis says they are the same, p val less than threshold, reject hyp, conclude that post is larger than pretest\n",
    "    print('Exact:','W=',w,'pvalue=',p)\n",
    "    a,b = wilcoxon(r,o,mode=\"exact\", alternative=\"greater\")\n",
    "    print('Greater:','W=',a,'pvalue=',b)\n",
    "\n",
    "# display(learning_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with learning_pickle_file.open('wb') as handle:\n",
    "    pickle.dump(learn_df, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "print('Saved learning table to {}'.format(learning_pickle_file))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute the effect size (estimated by Cliff's delta)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = ['span', 'mst', 'error']\n",
    "\n",
    "for name in categories:\n",
    "    r = list(df['pretest_{}'.format(name)])\n",
    "    o = list(df['posttest_{}'.format(name)])\n",
    "    print(name)\n",
    "    \n",
    "    # Estimate effect size by Cliff's Delta.\n",
    "    d = two_group_difference(control=r, test=o, effect_size='cliffs_delta')\n",
    "    print(\"For categury {} Cliff's delta = {}\".format(name, d)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "justhink-analysis-venv",
   "language": "python",
   "name": "justhink-analysis-venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "201px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
